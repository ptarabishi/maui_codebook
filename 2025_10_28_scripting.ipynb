{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efff2b78",
   "metadata": {},
   "source": [
    "# to do: \n",
    " \n",
    "from preprocess notebook\n",
    "- [x] write code for looping through unprocessed files\n",
    "- [x] motion correction script\n",
    "    - fixed nii\n",
    "- [x] clustering script\n",
    "    - labels.h5 - cluster labels\n",
    "    - supervoxel.h5 - ca_signal, brain dimensions\n",
    "- [x] fictrac processing script\n",
    "    - behavior.h5 - fictrac data, camera_fr, scope_fr\n",
    "- clean up ttl calculations? nah\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ba21b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:54:34.908721Z",
     "start_time": "2025-11-12T18:54:34.904003Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "from src import io, moco, roi, ttl, zdF\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# import nibabel as nib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f568a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T00:32:31.244104Z",
     "start_time": "2025-11-06T00:32:00.606953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed experiments: ['251014_1701', '251014_1801', '251024_2001']\n",
      "unprocessed experiments: ['251024_1901']\n",
      "working on: 251024_1901\n",
      "    made processed directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     struc_channel \u001b[38;5;241m=\u001b[39m func_channel\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# loads in ENTIRE niis\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m func_data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_nii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_channel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m struc_data \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mload_nii(struc_channel)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# loads in partial niis\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# func_nii = nib.load(func_channel)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# func_data = func_nii.dataobj[...,:50]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# struc_data = func_data\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/maui_codebook/src/io.py:16\u001b[0m, in \u001b[0;36mload_nii\u001b[0;34m(fpath)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"loads volume from filepath and returns N-Dim numpy array\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m LOG\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m imgarray \u001b[38;5;241m=\u001b[39m \u001b[43mants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m volume \u001b[38;5;241m=\u001b[39m imgarray\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     18\u001b[0m LOG\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvolume\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/maui/lib/python3.9/site-packages/ants/core/ants_image_io.py:363\u001b[0m, in \u001b[0;36mimage_read\u001b[0;34m(filename, dimension, pixeltype, reorient)\u001b[0m\n\u001b[1;32m    360\u001b[0m     ants_image \u001b[38;5;241m=\u001b[39m ants\u001b[38;5;241m.\u001b[39mfrom_pointer(itk_pointer)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixeltype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         ants_image \u001b[38;5;241m=\u001b[39m \u001b[43mants_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixeltype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (reorient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (ants_image\u001b[38;5;241m.\u001b[39mdimension \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reorient \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/maui/lib/python3.9/site-packages/ants/decorators.py:7\u001b[0m, in \u001b[0;36mimage_method.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func) \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/maui/lib/python3.9/site-packages/ants/core/ants_image_io.py:498\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(image, pixeltype)\u001b[0m\n\u001b[1;32m    496\u001b[0m fn_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p2_short,ndim)\n\u001b[1;32m    497\u001b[0m libfn \u001b[38;5;241m=\u001b[39m get_lib_fn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mantsImageClone\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mfn_suffix)\n\u001b[0;32m--> 498\u001b[0m pointer_cloned \u001b[38;5;241m=\u001b[39m \u001b[43mlibfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpointer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ants\u001b[38;5;241m.\u001b[39mfrom_pointer(pointer_cloned)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set main data path\n",
    "base_data_path = \"/Volumes/AhmedLab/princess/data\"\n",
    "\n",
    "# loop over all directories within princess/data\n",
    "processed_exps = []\n",
    "unprocessed_exps = []\n",
    "for folder_name in os.listdir(os.path.join(base_data_path,'raw')):\n",
    "\n",
    "    raw_path = os.path.join(base_data_path, 'raw', folder_name)\n",
    "    processed_path = os.path.join(base_data_path, 'processed', folder_name)\n",
    "\n",
    "    # do not run on .DS_Store folder\n",
    "    if folder_name != \".DS_Store\":\n",
    "        # for folders that have a processed + raw folder\n",
    "        # add to processed experiments list\n",
    "        if os.path.isdir(raw_path) and os.path.isdir(processed_path):\n",
    "            processed_exps.append(folder_name)\n",
    "        else:\n",
    "        # on directories that do not have a processed directory\n",
    "            unprocessed_exps.append(folder_name)\n",
    "\n",
    "print(f'processed experiments: {processed_exps}')\n",
    "print(f'unprocessed experiments: {unprocessed_exps}')\n",
    "\n",
    "for exp in unprocessed_exps:\n",
    "    print(f'working on: {exp}')\n",
    "    path = os.path.join(base_data_path, 'raw', exp)\n",
    "\n",
    "    # make processed directory\n",
    "    processed_path = os.path.join(base_data_path, 'processed', exp)\n",
    "    os.mkdir(processed_path)\n",
    "    print('    made processed directory')\n",
    "\n",
    "    # load in functional and structural data\n",
    "    if glob.glob(os.path.join(path, '*channel_2.nii')):\n",
    "        func_channel = glob.glob(os.path.join(path, '*channel_2.nii'))[0]\n",
    "        struc_channel = glob.glob(os.path.join(path, '*channel_1.nii'))[0]\n",
    "    else:\n",
    "        # if there is only data from a single channel\n",
    "        func_channel = glob.glob(os.path.join(path, '*channel_1.nii'))[0]\n",
    "        # use the functional data to generate fixed brain\n",
    "        struc_channel = func_channel\n",
    "\n",
    "    # loads in ENTIRE niis\n",
    "    func_data = io.load_nii(func_channel)\n",
    "    struc_data = io.load_nii(struc_channel)\n",
    "\n",
    "    # loads in partial niis\n",
    "    # func_nii = nib.load(func_channel)\n",
    "    # func_data = func_nii.dataobj[...,:50]\n",
    "    # struc_data = func_data\n",
    "\n",
    "    dimensions = pd.DataFrame(func_data.shape)\n",
    "    print('    running motion correction')\n",
    "    # generate fixed brain\n",
    "    mean_brain, fixed_brain = moco.generate_fixed(struc_data, struc_data.shape[-1])\n",
    "    io.save_nii(f'{processed_path}/fixed.nii', mean_brain)\n",
    "\n",
    "    # run motion correction on functional data and save out motion corrected brain\n",
    "    moco_func_brain = moco.motion_correction(func_data, fixed_brain)\n",
    "    io.save_nii(f'{processed_path}/moco_brain.nii', moco_func_brain)\n",
    "\n",
    "    print('    clustering pixels')\n",
    "\n",
    "    n_clusters = 1000\n",
    "    cluster_labels = roi.extract_ROIs(moco_func_brain, n_clusters)\n",
    "    print('    calculating df/F signal')\n",
    "    df = zdF.calculate_zscoredF(moco_func_brain, cluster_labels, n_clusters)\n",
    "\n",
    "    # make cluster + zdF h5\n",
    "\n",
    "    hf = h5py.File(f'{processed_path}/{n_clusters}_signals.h5', 'w')\n",
    "    hf.create_dataset('labels', data=cluster_labels)\n",
    "    hf.create_dataset('df/f', data=df)\n",
    "    hf.close()\n",
    "\n",
    "    # load in csv\n",
    "    csv_file = glob.glob(os.path.join(path, '*csv'))[0]\n",
    "\n",
    "    print('    saving acquisition parameters')\n",
    "    ttls = ttl.read_csv(csv_file)\n",
    "    scope_timestamps = ttl.extract_2p_relative_timestamps(ttls)\n",
    "    camera_timestamps = ttl.extract_camera_relative_timestamps(ttls)\n",
    "    scope_framerate = ttl.get_frame_rate(pd.Series(scope_timestamps))\n",
    "    camera_framerate = ttl.get_frame_rate(pd.Series(camera_timestamps))\n",
    "\n",
    "    hf_path = f'{processed_path}/acquisition_parameters.h5'\n",
    "    with h5py.File(hf_path,'w') as hf:\n",
    "        hf.create_dataset('scope_fr', data=scope_framerate)\n",
    "        hf.create_dataset('camera_fr', data=camera_framerate)\n",
    "        hf.create_dataset('brain_dimensions', data = dimensions)\n",
    "    hf.close()\n",
    "    camera_framerate = 170\n",
    "    # load fictrac data\n",
    "    dat_file = glob.glob(os.path.join(path, '*.dat'))[0]\n",
    "    fictrac_data = pd.DataFrame(pd.read_csv(dat_file, header=None))\n",
    "\n",
    "    print('    smoothing and saving fictrac speed')\n",
    "    win_size = int(.5 * camera_framerate)\n",
    "    inst_speed = np.rad2deg(fictrac_data[18])\n",
    "    smoothed_speed = savgol_filter(inst_speed, win_size, 3)\n",
    "\n",
    "    xy_pos = pd.DataFrame({'x': fictrac_data[14], 'y': fictrac_data[15]})\n",
    "    delta_rot = pd.DataFrame({'x': np.rad2deg(fictrac_data[5]), 'y': np.rad2deg(fictrac_data[7]), 'z' : np.rad2deg(fictrac_data[7])})\n",
    "\n",
    "    hf_path = f'{processed_path}/fictrac.h5'\n",
    "    with h5py.File(hf_path,'w') as hf:\n",
    "        hf.create_dataset('fictrac_time', data = fictrac_data[0])\n",
    "        hf.create_dataset('smoothed_speed', data=smoothed_speed)\n",
    "        hf.create_dataset('2d_pos', data = xy_pos)\n",
    "        hf.create_dataset('delta_rot', data = delta_rot)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b65be1b28aa739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T23:52:48.162406Z",
     "start_time": "2025-11-05T23:52:48.156636Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_fictrac_h5(raw_path, processed_path):\n",
    "    dat_file = glob.glob(os.path.join(raw_path, '*.dat'))[0]\n",
    "    fictrac_data = pd.DataFrame(pd.read_csv(dat_file, header=None))\n",
    "\n",
    "    print('    smoothing and saving fictrac speed')\n",
    "    win_size = int(.5 * camera_framerate)\n",
    "    inst_speed = np.rad2deg(fictrac_data[18])\n",
    "    smoothed_speed = savgol_filter(inst_speed, win_size, 3)\n",
    "\n",
    "    xy_pos = pd.DataFrame({'x': fictrac_data[14], 'y': fictrac_data[15]})\n",
    "    delta_rot = pd.DataFrame({'x': np.rad2deg(fictrac_data[5]), 'y': np.rad2deg(fictrac_data[7]), 'z' : np.rad2deg(fictrac_data[7])})\n",
    "\n",
    "    hf_path = f'{processed_path}/fictrac.h5'\n",
    "    with h5py.File(hf_path,'w') as hf:\n",
    "        hf.create_dataset('fictrac_time', data = fictrac_data[0])\n",
    "        hf.create_dataset('smoothed_speed', data=smoothed_speed)\n",
    "        hf.create_dataset('2d_pos', data = xy_pos)\n",
    "        hf.create_dataset('delta_rot', data = delta_rot)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3325b06f879dbb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:54:48.564368Z",
     "start_time": "2025-11-12T18:54:48.439050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/AhmedLab/princess/data/processed/251024_1901\n"
     ]
    }
   ],
   "source": [
    "base_data_path = \"/Volumes/AhmedLab/princess/data\"\n",
    "exp = 1901\n",
    "\n",
    "# set experiment file path\n",
    "raw_path = glob.glob(os.path.join(base_data_path, 'raw', f'*{exp}'))[0]\n",
    "processed_path = glob.glob(os.path.join(base_data_path, 'processed', f'*{exp}'))[0]\n",
    "print(processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623738227dbcdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_supervoxels(processed_path, brain_signal:np.ndarray, n_clusters: int): \n",
    "    \"\"\"\n",
    "    Outputs an h5 containing supervoxel ids and signal timeseries\n",
    "    \n",
    "    inputs: \"\"\"\n",
    "    cluster_labels = roi.extract_ROIs(brain_signal, n_clusters)\n",
    "    print('    calculating df/F signal')\n",
    "    df = zdF.calculate_zscoredF(brain_signal, cluster_labels, n_clusters)\n",
    "\n",
    "    # make cluster + zdF h5\n",
    "    hf = h5py.File(f'{processed_path}/{n_clusters}_signals.h5', 'w')\n",
    "    hf.create_dataset('labels', data=cluster_labels)\n",
    "    hf.create_dataset('df/f', data=df)\n",
    "    hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
