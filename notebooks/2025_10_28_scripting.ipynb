{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efff2b78",
   "metadata": {},
   "source": [
    "# to do: \n",
    " \n",
    "from preprocess notebook\n",
    "- [x] write code for looping through unprocessed files\n",
    "- [x] motion correction script\n",
    "    - fixed nii\n",
    "- [x] clustering script\n",
    "    - labels.h5 - cluster labels\n",
    "    - supervoxel.h5 - ca_signal, brain dimensions\n",
    "- [x] fictrac processing script\n",
    "    - behavior.h5 - fictrac data, camera_fr, scope_fr\n",
    "- clean up ttl calculations? nah\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "64ba21b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T19:41:43.097901Z",
     "start_time": "2025-11-21T19:41:42.606114Z"
    }
   },
   "source": [
    "import glob\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "from src import io, moco, roi, ttl, zdF\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# import nibabel as nib\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6f568a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T19:41:43.172014Z",
     "start_time": "2025-11-21T19:41:43.102873Z"
    }
   },
   "source": [
    "# set main data path\n",
    "base_data_path = \"/Volumes/AhmedLab/princess/data\"\n",
    "\n",
    "# loop over all directories within princess/data\n",
    "processed_exps = []\n",
    "unprocessed_exps = []\n",
    "for folder_name in os.listdir(os.path.join(base_data_path,'raw')):\n",
    "\n",
    "    raw_path = os.path.join(base_data_path, 'raw', folder_name)\n",
    "    processed_path = os.path.join(base_data_path, 'processed', folder_name)\n",
    "\n",
    "    # do not run on .DS_Store folder\n",
    "    if folder_name != \".DS_Store\":\n",
    "        # for folders that have a processed + raw folder\n",
    "        # add to processed experiments list\n",
    "        if os.path.isdir(raw_path) and os.path.isdir(processed_path):\n",
    "            processed_exps.append(folder_name)\n",
    "        else:\n",
    "        # on directories that do not have a processed directory\n",
    "            unprocessed_exps.append(folder_name)\n",
    "\n",
    "print(f'processed experiments: {processed_exps}')\n",
    "print(f'unprocessed experiments: {unprocessed_exps}')\n",
    "\n",
    "for exp in unprocessed_exps:\n",
    "    print(f'working on: {exp}')\n",
    "    path = os.path.join(base_data_path, 'raw', exp)\n",
    "\n",
    "    # make processed directory\n",
    "    processed_path = os.path.join(base_data_path, 'processed', exp)\n",
    "    os.mkdir(processed_path)\n",
    "    print('    made processed directory')\n",
    "\n",
    "    # load in functional and structural data\n",
    "    if glob.glob(os.path.join(path, '*channel_2.nii')):\n",
    "        func_channel = glob.glob(os.path.join(path, '*channel_2.nii'))[0]\n",
    "        struc_channel = glob.glob(os.path.join(path, '*channel_1.nii'))[0]\n",
    "    else:\n",
    "        # if there is only data from a single channel\n",
    "        func_channel = glob.glob(os.path.join(path, '*channel_1.nii'))[0]\n",
    "        # use the functional data to generate fixed brain\n",
    "        struc_channel = func_channel\n",
    "\n",
    "    # loads in ENTIRE niis\n",
    "    func_data = io.load_nii(func_channel)\n",
    "    struc_data = io.load_nii(struc_channel)\n",
    "\n",
    "    # loads in partial niis\n",
    "    # func_nii = nib.load(func_channel)\n",
    "    # func_data = func_nii.dataobj[...,:50]\n",
    "    # struc_data = func_data\n",
    "\n",
    "    dimensions = pd.DataFrame(func_data.shape)\n",
    "    print('    running motion correction')\n",
    "    # generate fixed brain\n",
    "    mean_brain, fixed_brain = moco.generate_fixed(struc_data, struc_data.shape[-1])\n",
    "    io.save_nii(f'{processed_path}/fixed.nii', mean_brain)\n",
    "\n",
    "    # run motion correction on functional data and save out motion corrected brain\n",
    "    moco_func_brain = moco.motion_correction(func_data, fixed_brain)\n",
    "    io.save_nii(f'{processed_path}/moco_brain.nii', moco_func_brain)\n",
    "\n",
    "    print('    clustering pixels')\n",
    "\n",
    "    n_clusters = 1000\n",
    "    cluster_labels = roi.extract_ROIs(moco_func_brain, n_clusters)\n",
    "    print('    calculating df/F signal')\n",
    "    df = zdF.calculate_zscoredF(moco_func_brain, cluster_labels, n_clusters)\n",
    "\n",
    "    # make cluster + zdF h5\n",
    "\n",
    "    hf = h5py.File(f'{processed_path}/{n_clusters}_signals.h5', 'w')\n",
    "    hf.create_dataset('labels', data=cluster_labels)\n",
    "    hf.create_dataset('df/f', data=df)\n",
    "    hf.close()\n",
    "\n",
    "    # load in csv\n",
    "    csv_file = glob.glob(os.path.join(path, '*csv'))[0]\n",
    "\n",
    "    print('    saving acquisition parameters')\n",
    "    ttls = ttl.read_csv(csv_file)\n",
    "    scope_timestamps = ttl.extract_2p_relative_timestamps(ttls)\n",
    "    camera_timestamps = ttl.extract_camera_relative_timestamps(ttls)\n",
    "    scope_framerate = ttl.get_frame_rate(pd.Series(scope_timestamps))\n",
    "    camera_framerate = ttl.get_frame_rate(pd.Series(camera_timestamps))\n",
    "\n",
    "    hf_path = f'{processed_path}/acquisition_parameters.h5'\n",
    "    with h5py.File(hf_path,'w') as hf:\n",
    "        hf.create_dataset('scope_fr', data=scope_framerate)\n",
    "        hf.create_dataset('camera_fr', data=camera_framerate)\n",
    "        hf.create_dataset('brain_dimensions', data = dimensions)\n",
    "    hf.close()\n",
    "    camera_framerate = 170\n",
    "    # load fictrac data\n",
    "    dat_file = glob.glob(os.path.join(path, '*.dat'))[0]\n",
    "    fictrac_data = pd.DataFrame(pd.read_csv(dat_file, header=None))\n",
    "\n",
    "    print('    smoothing and saving fictrac speed')\n",
    "    win_size = int(.5 * camera_framerate)\n",
    "    inst_speed = np.rad2deg(fictrac_data[18])\n",
    "    smoothed_speed = savgol_filter(inst_speed, win_size, 3)\n",
    "\n",
    "    xy_pos = pd.DataFrame({'x': fictrac_data[14], 'y': fictrac_data[15]})\n",
    "    delta_rot = pd.DataFrame({'x': np.rad2deg(fictrac_data[5]), 'y': np.rad2deg(fictrac_data[7]), 'z' : np.rad2deg(fictrac_data[7])})\n",
    "\n",
    "    hf_path = f'{processed_path}/fictrac.h5'\n",
    "    with h5py.File(hf_path,'w') as hf:\n",
    "        hf.create_dataset('fictrac_time', data = fictrac_data[0])\n",
    "        hf.create_dataset('smoothed_speed', data=smoothed_speed)\n",
    "        hf.create_dataset('2d_pos', data = xy_pos)\n",
    "        hf.create_dataset('delta_rot', data = delta_rot)\n",
    "    hf.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed experiments: ['251014_1701', '251014_1801', '251024_1901', '251024_2001']\n",
      "unprocessed experiments: []\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b65be1b28aa739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T23:52:48.162406Z",
     "start_time": "2025-11-05T23:52:48.156636Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_fictrac_h5(raw_path, processed_path):\n",
    "    dat_file = glob.glob(os.path.join(raw_path, '*.dat'))[0]\n",
    "    fictrac_data = pd.DataFrame(pd.read_csv(dat_file, header=None))\n",
    "\n",
    "    print('    smoothing and saving fictrac speed')\n",
    "    win_size = int(.5 * camera_framerate)\n",
    "    inst_speed = np.rad2deg(fictrac_data[18])\n",
    "    smoothed_speed = savgol_filter(inst_speed, win_size, 3)\n",
    "\n",
    "    xy_pos = pd.DataFrame({'x': fictrac_data[14], 'y': fictrac_data[15]})\n",
    "    delta_rot = pd.DataFrame({'x': np.rad2deg(fictrac_data[5]), 'y': np.rad2deg(fictrac_data[7]), 'z' : np.rad2deg(fictrac_data[7])})\n",
    "\n",
    "    hf_path = f'{processed_path}/fictrac.h5'\n",
    "    with h5py.File(hf_path,'w') as hf:\n",
    "        hf.create_dataset('fictrac_time', data = fictrac_data[0])\n",
    "        hf.create_dataset('smoothed_speed', data=smoothed_speed)\n",
    "        hf.create_dataset('2d_pos', data = xy_pos)\n",
    "        hf.create_dataset('delta_rot', data = delta_rot)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3325b06f879dbb30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T18:54:48.564368Z",
     "start_time": "2025-11-12T18:54:48.439050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/AhmedLab/princess/data/processed/251024_1901\n"
     ]
    }
   ],
   "source": [
    "base_data_path = \"/Volumes/AhmedLab/princess/data\"\n",
    "exp = 1901\n",
    "\n",
    "# set experiment file path\n",
    "raw_path = glob.glob(os.path.join(base_data_path, 'raw', f'*{exp}'))[0]\n",
    "processed_path = glob.glob(os.path.join(base_data_path, 'processed', f'*{exp}'))[0]\n",
    "print(processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623738227dbcdb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_supervoxels(processed_path, brain_signal:np.ndarray, n_clusters: int): \n",
    "    \"\"\"\n",
    "    Outputs an h5 containing supervoxel ids and signal timeseries\n",
    "    \n",
    "    inputs: \"\"\"\n",
    "    cluster_labels = roi.extract_ROIs(brain_signal, n_clusters)\n",
    "    print('    calculating df/F signal')\n",
    "    df = zdF.calculate_zscoredF(brain_signal, cluster_labels, n_clusters)\n",
    "\n",
    "    # make cluster + zdF h5\n",
    "    hf = h5py.File(f'{processed_path}/{n_clusters}_signals.h5', 'w')\n",
    "    hf.create_dataset('labels', data=cluster_labels)\n",
    "    hf.create_dataset('df/f', data=df)\n",
    "    hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
